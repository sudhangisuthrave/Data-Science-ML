# -*- coding: utf-8 -*-
"""diabetes_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FeAnpioHrUwj0DCvyjEVclnWfVtZeGMj
"""

# install pyspark
! pip install pyspark

# create Spark session. If it already exists then Get it or Create a new one.
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("spark").getOrCreate()

! git clone https://github.com/education454/diabetes_dataset

! ls diabetes_dataset

# Create dataframe in spark.
# Panda dataframe are Mutable, meaning they can be changed.
# Spark Dataframe are immutable, meaning meaning once created, a Spark DataFrame cannot be directly modified. Transformations create new DataFrames.
df = spark.read.csv('/content/diabetes_dataset/diabetes.csv', header=True, inferSchema=True)

df.show()

# Print the dataframe's schema
df.printSchema()

# in df, count gives # of rows and df.columns gives the # of columns
print((df.count(), len(df.columns)))

df.groupBy('Outcome').count().show()

# describe() gives the details of each column like count, mean, stddeviation, min and max values
df.describe().show()

# find null values
for col in df.columns:
  print (col+ ":", df[df[col].isNull()].count())

# For all the columns find total number of 0 entries
def count_zeros():
  columns_list = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
  for i in columns_list:
    print(i+":", df[df[i]==0].count())
count_zeros()

# Find mean values for all the columns
from pyspark.sql.functions import *
for i in df.columns[1:6]:
  data = df.agg({i:'mean'}).first()[0]
  print("Mean value for {} is {}".format(i,int(data)))
  df = df.withColumn(i,when(df[i]==0,int(data)).otherwise(df[i]))

df.show()

for col in df.columns:
  print("Coorelation to the final Outcome for {} is {}".format(col,df.stat.corr('Outcome',col)))

# Create a Vector assembler by comn=bining all the columns together as one data point
from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI','DiabetesPedigreeFunction', 'Age'],outputCol='features')

output_data = assembler.transform(df)

output_data.printSchema()

output_data.show()

# Logistic Regression classification based on sigmoid function to output a probability, which is then used to assign the data point to one of two classes
from pyspark.ml.classification import LogisticRegression
final_data = output_data.select('features', 'Outcome')

final_data.printSchema()

#create training and test data by spliting in 70% of training data and 30% of test data
train, test = final_data.randomSplit([0.7, 0.3])

models = LogisticRegression(labelCol='Outcome')
model = models.fit(train)
summary=model.summary
summary.predictions.describe().show()

# Evaluation and Test Model
# Binary Classification Model which give results on 1/0, true/false, spam/not spam format. Just 2 possible outputs
from pyspark.ml.evaluation import BinaryClassificationEvaluator
predictions = model.evaluate(test)

predictions.predictions.show(20)

# use Binary classification evaluator to evaluate the model. It shows 82.33% accuracy
evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='Outcome')
evaluator.evaluate(model.transform(test))

# save the model for future use
model.save("model")

from pyspark.ml.classification import LogisticRegressionModel
# load the logistic regression model created for use
model = LogisticRegressionModel.load('model')